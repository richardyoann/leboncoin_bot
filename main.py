#!/usr/bin/env python3
"""
Script principal pour le scraping avanc√©.

Usage:
    python main.py [--config CONFIG_PATH] [--headless] [--dry-run]
"""

import argparse
import sys
from pathlib import Path
from rich.console import Console
from rich.table import Table
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn
from rich.panel import Panel
import signal

# Ajout du chemin src pour les imports
sys.path.append(str(Path(__file__).parent / "src"))

from core.scraper import AdvancedScraper
from exporters.data_exporter import DataExporter
from utils.logger import setup_logger
from core.exceptions import ScrapingError, ConfigurationError

# Configuration du logger
logger = setup_logger("main")
console = Console()

class ScrapingApp:
    """Application principale de scraping."""
    
    def __init__(self):
        self.scraper = None
        self.interrupted = False
        
        # Gestionnaire d'interruption propre
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
    
    def _signal_handler(self, signum, frame):
        """Gestionnaire d'interruption propre."""
        console.print("\n[yellow]‚ö†Ô∏è  Interruption d√©tect√©e - arr√™t propre en cours...[/yellow]")
        self.interrupted = True
        if self.scraper and hasattr(self.scraper, 'browser_manager'):
            self.scraper.browser_manager.close()
        sys.exit(0)
    
    def display_welcome(self):
        """Affiche le message d'accueil."""
        welcome_text = """
üï∑Ô∏è  [bold blue]Advanced Web Scraper v2.0[/bold blue]

[dim]Un scraper robuste et √©thique pour l'extraction de donn√©es web[/dim]

‚ö° Fonctionnalit√©s principales:
  ‚Ä¢ Gestion intelligente des CAPTCHAs
  ‚Ä¢ D√©lais adaptatifs anti-d√©tection  
  ‚Ä¢ Export multi-formats (JSON, CSV, Excel)
  ‚Ä¢ Logging d√©taill√© et monitoring
  ‚Ä¢ Configuration flexible via YAML
        """
        console.print(Panel(welcome_text, border_style="blue"))
    
    def display_config_summary(self, scraper: AdvancedScraper):
        """Affiche un r√©sum√© de la configuration."""
        config = scraper.config
        
        table = Table(title="üìã Configuration de scraping", show_header=True)
        table.add_column("Param√®tre", style="cyan")
        table.add_column("Valeur", style="green")
        
        # Informations principales
        table.add_row("üéØ Cibles", str(len(config['targets'])))
        table.add_row("üìÑ Pages max/cible", str(config['scraping']['max_pages']))
        table.add_row("üï∂Ô∏è  Mode headless", "Oui" if config['scraping']['headless'] else "Non")
        table.add_row("‚è±Ô∏è  D√©lai min", f"{config['timing']['min_delay_between_requests']}s")
        table.add_row("‚è±Ô∏è  D√©lai max", f"{config['timing']['max_delay_between_requests']}s")
        table.add_row("üîÑ Retry max", str(config['limits']['max_retries']))
        
        # D√©tail des cibles
        for i, target in enumerate(config['targets'], 1):
            table.add_row(
                f"   Cible {i}",
                f"{target['name']} ({len(target['keywords'])} mots-cl√©s)"
            )
        
        console.print(table)
    
    def display_results_summary(self, ads, stats):
        """Affiche un r√©sum√© des r√©sultats."""
        # Tableau principal
        table = Table(title="üìä R√©sultats du scraping", show_header=True)
        table.add_column("M√©trique", style="cyan")
        table.add_column("Valeur", style="green")
        
        table.add_row("üéØ Total annonces", str(len(ads)))
        table.add_row("‚úÖ Pages r√©ussies", str(stats['successful_pages']))
        table.add_row("‚ùå Pages √©chou√©es", str(stats['failed_pages']))
        table.add_row("üìä Taux de r√©ussite", f"{stats['success_rate']:.1f}%")
        table.add_row("üïê Dur√©e totale", f"{stats['duration_seconds']:.0f}s")
        table.add_row("ü§ñ CAPTCHAs", str(stats['captcha_encounters']))
        
        console.print(table)
        
        # Analyse des prix si disponible
        if ads:
            prices = [ad.clean_price for ad in ads if ad.clean_price and ad.clean_price > 0]
            if prices:
                console.print("\n[bold]üí∞ Analyse des prix:[/bold]")
                console.print(f"  ‚Ä¢ Prix moyen: {sum(prices)/len(prices):.2f}‚Ç¨")
                console.print(f"  ‚Ä¢ Prix m√©dian: {sorted(prices)[len(prices)//2]:.2f}‚Ç¨")
                console.print(f"  ‚Ä¢ Prix min/max: {min(prices):.2f}‚Ç¨ - {max(prices):.2f}‚Ç¨")
    
    def run(self, config_path: str, dry_run: bool = False):
        """Lance l'application de scraping."""
        try:
            # Initialisation
            self.display_welcome()
            
            console.print("[yellow]üîß Initialisation du scraper...[/yellow]")
            self.scraper = AdvancedScraper(config_path)
            
            # Affichage de la configuration
            self.display_config_summary(self.scraper)
            
            if dry_run:
                console.print("\n[blue]‚ÑπÔ∏è  Mode dry-run activ√© - aucun scraping effectu√©[/blue]")
                return
            
            # Confirmation utilisateur
            if not console.input("\n[bold]Continuer avec le scraping ? [y/N]: [/bold]").lower().startswith('y'):
                console.print("[yellow]Scraping annul√© par l'utilisateur[/yellow]")
                return
            
            # Scraping avec barre de progression
            console.print("\n[green]üöÄ D√©but du scraping...[/green]")
            
            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                BarColumn(),
                TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
                console=console
            ) as progress:
                
                task = progress.add_task("Scraping en cours...", total=100)
                
                # Lancement du scraping
                ads = self.scraper.scrape_all_targets()
                progress.update(task, completed=100)
            
            # Statistiques
            stats = self.scraper.get_session_stats()
            
            # Affichage des r√©sultats
            console.print("\n[green]‚úÖ Scraping termin√©![/green]")
            self.display_results_summary(ads, stats)
            
            # Export des donn√©es
            if ads:
                console.print("\n[blue]üíæ Export des donn√©es...[/blue]")
                exporter = DataExporter()
                
                # Export dans tous les formats
                files = exporter.export_all_formats(ads)
                
                console.print("\n[green]üìÅ Fichiers cr√©√©s:[/green]")
                for format_type, filepath in files.items():
                    console.print(f"  ‚Ä¢ {format_type.upper()}: {filepath}")
                
                # G√©n√©ration du rapport
                report = exporter.generate_report(ads, stats)
                console.print(f"\n[blue]üìã Rapport disponible dans les exports[/blue]")
            else:
                console.print("\n[yellow]‚ö†Ô∏è  Aucune donn√©e √† exporter[/yellow]")
            
        except ConfigurationError as e:
            console.print(f"[red]‚ùå Erreur de configuration: {e}[/red]")
            sys.exit(1)
        except ScrapingError as e:
            console.print(f"[red]‚ùå Erreur de scraping: {e}[/red]")
            sys.exit(1)
        except KeyboardInterrupt:
            console.print("\n[yellow]‚ö†Ô∏è  Scraping interrompu par l'utilisateur[/yellow]")
        except Exception as e:
            logger.exception("Erreur inattendue")
            console.print(f"[red]üí• Erreur inattendue: {e}[/red]")
            sys.exit(1)

def main():
    """Point d'entr√©e principal."""
    parser = argparse.ArgumentParser(
        description="Advanced Web Scraper - Solution robuste de scraping"
    )
    parser.add_argument(
        "--config", 
        default="config/config.yaml",
        help="Chemin vers le fichier de configuration (d√©faut: config/config.yaml)"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Mode test - affiche la configuration sans scraper"
    )
    parser.add_argument(
        "--headless",
        action="store_true", 
        help="Force le mode headless"
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="Mode verbose (debug)"
    )
    
    args = parser.parse_args()
    
    # Configuration du niveau de log
    if args.verbose:
        logger.setLevel("DEBUG")
    
    # V√©rification du fichier de configuration
    if not Path(args.config).exists():
        console.print(f"[red]‚ùå Fichier de configuration non trouv√©: {args.config}[/red]")
        sys.exit(1)
    
    # Lancement de l'application
    app = ScrapingApp()
    app.run(args.config, args.dry_run)

if __name__ == "__main__":
    main()